{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import cifar10\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# CIFAR_10 is a set of 60K images 32x32 pixels on 3 channels\n",
    "\n",
    "IMG_CHANNELS = 3\n",
    "IMG_ROWS = 32\n",
    "IMG_COLS = 32\n",
    "\n",
    "#constant\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "NB_EPOCH = 20\n",
    "NB_CLASSES = 10\n",
    "VERBOSE = 1\n",
    "VALIDATION_SPLIT = 0.2\n",
    "OPTIM = RMSprop()\n",
    "\n",
    "#load dataset\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "print('X_train shape:', X_train.shape)\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "\n",
    "# convert to categorical\n",
    "\n",
    "Y_train = np_utils.to_categorical(y_train, NB_CLASSES)\n",
    "Y_test = np_utils.to_categorical(y_test, NB_CLASSES)\n",
    "\n",
    "# float and normalization\n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "# network\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), padding='same',\n",
    "input_shape=(IMG_ROWS, IMG_COLS, IMG_CHANNELS)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(32, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64, 3, 3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(NB_CLASSES))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# train\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=OPTIM,\n",
    "metrics=['accuracy'])\n",
    "model.fit(X_train, Y_train, batch_size=BATCH_SIZE,\n",
    "epochs=NB_EPOCH, validation_split=VALIDATION_SPLIT,\n",
    "verbose=VERBOSE)\n",
    "score = model.evaluate(X_test, Y_test,\n",
    "batch_size=BATCH_SIZE, verbose=VERBOSE)\n",
    "print(\"Test score:\", score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "#save model\n",
    "model_json = model.to_json()\n",
    "open('cifar10_architecture.json', 'w').write(model_json)\n",
    "model.save_weights('cifar10_weights.h5', overwrite=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "\n",
    "# Variables\n",
    "BATCH_SIZE = 32\n",
    "NB_EPOCH = 50\n",
    "VERBOSE = 1\n",
    "\n",
    "# Create preview directory\n",
    "if not os.path.exists('preview'):\n",
    "    os.makedirs('preview')\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# Convert class vectors to binary class matrices\n",
    "Y_train = to_categorical(y_train, 10)\n",
    "Y_test = to_categorical(y_test, 10)\n",
    "\n",
    "# Augment training set images\n",
    "print(\"Augmenting training set images...\")\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Fit the data generator\n",
    "datagen.fit(X_train)\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), padding='same', input_shape=X_train.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit_generator(datagen.flow(X_train, Y_train, batch_size=BATCH_SIZE),\n",
    "                              steps_per_epoch=X_train.shape[0] // BATCH_SIZE,\n",
    "                              epochs=NB_EPOCH, verbose=VERBOSE)\n",
    "\n",
    "# Evaluate the model\n",
    "score = model.evaluate(X_test, Y_test, batch_size=BATCH_SIZE, verbose=VERBOSE)\n",
    "print(\"Test score:\", score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "# Save model\n",
    "model_json = model.to_json()\n",
    "open('cifar10_architecture.json', 'w').write(model_json)\n",
    "model.save_weights('cifar10_weights.h5', overwrite=True)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of Ethical and Privacy Implications of Image Classification Algorithms\n",
    "\n",
    "While developing image classification algorithms like the one created in this exercise has significant potential for positive impact, it raises important ethical and privacy concerns.\n",
    "\n",
    "One primary concern is the potential for such algorithms to be used for facial recognition. The ability to accurately identify individuals from images has already been developed. Law enforcement agencies use it for purposes such as identifying suspects and tracking criminal activity. However, the use of facial recognition technology has been criticized for its potential to infringe on privacy rights and for its potential to perpetuate racial biases.\n",
    "\n",
    "### Facial Recognition and Privacy\n",
    "\n",
    "Facial recognition technology can be used to identify individuals without their knowledge or consent. However, this poses a significant risk to personal privacy, as it enables large-scale surveillance and tracking of individuals in public spaces. For example, governments and law enforcement agencies could use this technology to monitor political activists, journalists, or minority groups (Levinson-Waldman & DÃ­az, How to reform police monitoring of social media 2020).\n",
    "\n",
    "According to research by Joy Buolamwini, P.h.D., facial recognition technology has also been less accurate for people of color, women, and children (Dhinakaran, Coded bias: An insightful look at ai, algorithms and their risks to society 2023). In addition, in some states and many countries that use facial recognition technology, individuals are constantly monitored, and their movements are tracked without their knowledge or consent. This has resulted in misidentifications and false arrests.\n",
    "\n",
    "Additionally, the widespread use of facial recognition technology could lead to a surveillance state, with citizens constantly monitored and tracked by the government. This would severely limit people's freedom and privacy, as any action could be monitored and used against them.\n",
    "\n",
    "### Potential for Discrimination and Bias\n",
    "\n",
    "Another ethical issue with image classification algorithms is the potential for biased training data. Algorithms are only as good as the data they are trained on, and if the training data is biased, the algorithm will be as well. For example, large datasets may not represent human populations' diversity. An algorithm trained on images primarily featuring white people may be less accurate at identifying people of color. This can lead to disparities in the accuracy of facial recognition systems, exacerbate existing social inequalities, and lead to or justify discrimination.\n",
    "\n",
    "\n",
    "### Regulation and Oversight\n",
    "\n",
    "To ensure that image classification algorithms are developed and used ethically, it is essential to consider the potential implications carefully and implement safeguards to mitigate risks. This could include rigorous testing to ensure accuracy and fairness, and transparency in the development and use of algorithms. For example, an algorithm could be tested to ensure it performs equally well on images of people with different genders, skin tones, and age groups. This would avoid bias or prejudice in the results.\n",
    "\n",
    "Companies should also have processes to regularly review their algorithms and ensure they remain unbiased. Accountability measures should be established for algorithm developers and users to ensure any potential biases are addressed. Additionally, companies should be open to feedback from stakeholders and the public to ensure that any issues with algorithms are identified and managed in a timely manner.\n",
    "In conclusion, while the current algorithm is focused on classifying images of animals and vehicles, its potential extension to facial recognition raises important ethical and privacy concerns. Therefore, as advanced AI technologies continue to develop and deploy, it is crucial to consider their broader societal implications and establish mechanisms to ensure their responsible use.\n",
    "\n",
    "## References\n",
    "\n",
    "Levinson-Waldman, R., & DÃ­az, Ã. (2020, July 20). How to reform police monitoring of social media. Brookings. Retrieved March 20, 2023, from https://www.brookings.edu/techstream/how-to-reform-police-monitoring-of-social-media/ \n",
    "\n",
    "Dhinakaran, A. (2023, February 24). Coded bias: An insightful look at ai, algorithms and their risks to society. Forbes. Retrieved March 20, 2023, from https://www.forbes.com/sites/aparnadhinakaran/2021/04/15/coded-bias-an-insightful-look-at-ai-algorithms-and-their-risks-to-society/?sh=5c46425e2fd6 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
